{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as  np\ntest_df= pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntrain_df= pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:32:27.746547Z","iopub.execute_input":"2023-07-16T12:32:27.746986Z","iopub.status.idle":"2023-07-16T12:32:27.860338Z","shell.execute_reply.started":"2023-07-16T12:32:27.746945Z","shell.execute_reply":"2023-07-16T12:32:27.858837Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Reading Data","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:32:41.723806Z","iopub.execute_input":"2023-07-16T12:32:41.725342Z","iopub.status.idle":"2023-07-16T12:32:41.764473Z","shell.execute_reply.started":"2023-07-16T12:32:41.725254Z","shell.execute_reply":"2023-07-16T12:32:41.763014Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-07-16T12:58:59.233909Z","iopub.execute_input":"2023-07-16T12:58:59.234611Z","iopub.status.idle":"2023-07-16T12:58:59.253509Z","shell.execute_reply.started":"2023-07-16T12:58:59.234554Z","shell.execute_reply":"2023-07-16T12:58:59.252053Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3263 entries, 0 to 3262\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        3263 non-null   int64 \n 1   keyword   3237 non-null   object\n 2   location  2158 non-null   object\n 3   text      3263 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 102.1+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef clean(tweet): \n            \n    # Special characters\n    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"å_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\n    tweet = re.sub(r\"åÈ\", \"\", tweet)\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n    tweet = re.sub(r\"å¨\", \"\", tweet)\n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n    tweet = re.sub(r\"åÇ\", \"\", tweet)\n    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n    tweet = re.sub(r\"åÀ\", \"\", tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n            \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n \n    # Words with punctuations and special characters\n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        tweet = tweet.replace(p, f' {p} ')\n        \n    # ... and ..\n    tweet = tweet.replace('...', ' ... ')\n    if '...' not in tweet:\n        tweet = tweet.replace('..', ' ... ')      \n        \n    # Acronyms\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n    \n    # Grouping same words without embeddings\n    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n    \n    return tweet","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:00:50.571263Z","iopub.execute_input":"2023-07-16T13:00:50.571718Z","iopub.status.idle":"2023-07-16T13:00:50.764615Z","shell.execute_reply.started":"2023-07-16T13:00:50.571681Z","shell.execute_reply":"2023-07-16T13:00:50.763548Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef clean_text_without_nltk(text):\n    # Convert text to lowercase\n    text = text.lower()\n    \n    # Remove numbers\n#     text = re.sub(r'\\d+', '', text)\n    \n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    \n    # Remove extra whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n\n# train_df[\"text\"] = train_df[\"text\"].apply(clean_text_without_nltk)\n# test_df[\"text\"] = test_df[\"text\"].apply(clean_text_without_nltk)\n\ntrain_df[\"text\"] = train_df[\"text\"].apply(clean)\ntest_df[\"text\"] = test_df[\"text\"].apply(clean)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:01:13.235280Z","iopub.execute_input":"2023-07-16T13:01:13.235723Z","iopub.status.idle":"2023-07-16T13:08:13.507533Z","shell.execute_reply.started":"2023-07-16T13:01:13.235685Z","shell.execute_reply":"2023-07-16T13:08:13.505992Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"len(Target) == len(output)\nno Need for Data cleaning","metadata":{}},{"cell_type":"markdown","source":"### Prepare Data from model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df[\"text\"].to_numpy(),\n                                                                            train_df[\"target\"].to_numpy(),\n                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n                                                                            random_state=42)\ntrain_sentences, test_sentences, train_labels, test_labels = train_test_split(train_sentences,\n                                                                           train_labels,\n                                                                            test_size=0.2, # dedicate 10% of samples to validation set\n                                                                            random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:40.725358Z","iopub.execute_input":"2023-07-16T13:08:40.725792Z","iopub.status.idle":"2023-07-16T13:08:40.737101Z","shell.execute_reply.started":"2023-07-16T13:08:40.725751Z","shell.execute_reply":"2023-07-16T13:08:40.735660Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:52.621921Z","iopub.execute_input":"2023-07-16T13:08:52.623119Z","iopub.status.idle":"2023-07-16T13:08:52.631031Z","shell.execute_reply.started":"2023-07-16T13:08:52.623072Z","shell.execute_reply":"2023-07-16T13:08:52.629729Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"(5480, 5480, 762, 762)"},"metadata":{}}]},{"cell_type":"markdown","source":"### model results metrics","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score , precision_recall_fscore_support\ndef claclate_results_scores(y_true,y_pred):\n    model_accuarcy=accuracy_score(y_true,y_pred)\n    model_precision,model_recall,model_f_score,_=precision_recall_fscore_support(y_true,y_pred,average='weighted')\n    model_result= {\n        'model_accuarcy':model_accuarcy,\n        'model_precision':model_precision,\n        'model_recall':model_recall,\n        'model_f_score':model_f_score,\n        \n    }\n    return model_result","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:52.661038Z","iopub.execute_input":"2023-07-16T13:08:52.662208Z","iopub.status.idle":"2023-07-16T13:08:52.668240Z","shell.execute_reply.started":"2023-07-16T13:08:52.662146Z","shell.execute_reply":"2023-07-16T13:08:52.667291Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"### Sikit-learn model","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:52.685570Z","iopub.execute_input":"2023-07-16T13:08:52.686275Z","iopub.status.idle":"2023-07-16T13:08:52.691374Z","shell.execute_reply.started":"2023-07-16T13:08:52.686231Z","shell.execute_reply":"2023-07-16T13:08:52.690023Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"model_0 = Pipeline([\n                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n                    (\"clf\", MultinomialNB()) # model the text\n])\n\n# Fit the pipeline to the training data\n","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:52.711402Z","iopub.execute_input":"2023-07-16T13:08:52.712053Z","iopub.status.idle":"2023-07-16T13:08:52.717344Z","shell.execute_reply.started":"2023-07-16T13:08:52.712014Z","shell.execute_reply":"2023-07-16T13:08:52.715757Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"model_0= Pipeline([\n    ('tfidf',TfidfVectorizer()), # CONVERT TEXT TO NUM\n    ('classification',MultinomialNB()) # classiftion model layer\n])\nmodel_0.fit(train_sentences,train_labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:52.742890Z","iopub.execute_input":"2023-07-16T13:08:52.743556Z","iopub.status.idle":"2023-07-16T13:08:52.888946Z","shell.execute_reply.started":"2023-07-16T13:08:52.743518Z","shell.execute_reply":"2023-07-16T13:08:52.887581Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"Pipeline(steps=[('tfidf', TfidfVectorizer()),\n                ('classification', MultinomialNB())])"},"metadata":{}}]},{"cell_type":"code","source":"acc= model_0.score(val_sentences,val_labels)\nprint(f'model accuracy: {acc*100 :.2f}%')","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:52.891506Z","iopub.execute_input":"2023-07-16T13:08:52.891998Z","iopub.status.idle":"2023-07-16T13:08:52.918134Z","shell.execute_reply.started":"2023-07-16T13:08:52.891949Z","shell.execute_reply":"2023-07-16T13:08:52.916812Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"model accuracy: 77.43%\n","output_type":"stream"}]},{"cell_type":"code","source":"val_true=model_0.predict(val_sentences)\nbase_line_model=claclate_results_scores(val_true,val_labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:52.919722Z","iopub.execute_input":"2023-07-16T13:08:52.920826Z","iopub.status.idle":"2023-07-16T13:08:52.945451Z","shell.execute_reply.started":"2023-07-16T13:08:52.920762Z","shell.execute_reply":"2023-07-16T13:08:52.944194Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"### Deep learning model (TF)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:52.948918Z","iopub.execute_input":"2023-07-16T13:08:52.949816Z","iopub.status.idle":"2023-07-16T13:08:52.955637Z","shell.execute_reply.started":"2023-07-16T13:08:52.949748Z","shell.execute_reply":"2023-07-16T13:08:52.954261Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"train_sentences[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:52.957468Z","iopub.execute_input":"2023-07-16T13:08:52.957920Z","iopub.status.idle":"2023-07-16T13:08:52.972890Z","shell.execute_reply.started":"2023-07-16T13:08:52.957873Z","shell.execute_reply":"2023-07-16T13:08:52.971542Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"array(['whimsy as it pertains to mass casualties always impressive',\n       'diretube information ûò egypt cyprus and greece agreed to fightåêterrorism httptcov6ijxccd2i httptcoysxhfwmgod',\n       'storm damage reported in west tennessee httptco90l2lb5wmr',\n       'going to starbs its only 70 degrees part of the metro derailed its a beautiful morning in washington dc',\n       'clip topdown coercion the structural weakness ensuring government failure httptcognorijnsva'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"sequense_mean=round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))\nsequense_mean","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:52.977203Z","iopub.execute_input":"2023-07-16T13:08:52.978479Z","iopub.status.idle":"2023-07-16T13:08:52.993745Z","shell.execute_reply.started":"2023-07-16T13:08:52.978424Z","shell.execute_reply":"2023-07-16T13:08:52.992400Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"15"},"metadata":{}}]},{"cell_type":"markdown","source":"####","metadata":{}},{"cell_type":"markdown","source":"## Buliding Model","metadata":{}},{"cell_type":"markdown","source":"#### Creating callback function","metadata":{}},{"cell_type":"code","source":"def create_tensorboard_callback(dir_name, experiment_name):\n  \"\"\"\n  Creates a TensorBoard callback instand to store log files.\n\n  Stores log files with the filepath:\n    \"dir_name/experiment_name/current_datetime/\"\n\n  Args:\n    dir_name: target directory to store TensorBoard log files\n    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n  \"\"\"\n  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n      log_dir=log_dir\n  )\n  print(f\"Saving TensorBoard log files to: {log_dir}\")\n  return tensorboard_callback","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:52.995647Z","iopub.execute_input":"2023-07-16T13:08:52.996468Z","iopub.status.idle":"2023-07-16T13:08:53.006911Z","shell.execute_reply.started":"2023-07-16T13:08:52.996418Z","shell.execute_reply":"2023-07-16T13:08:53.005655Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\ndef saving_best_model(dir_name,monitor='val_accuracy'):\n    log_dir = \"Models\"  + \"/\" +dir_name\n\n    checkpoint = ModelCheckpoint(filepath=log_dir+'/'+dir_name+'.h5', \n                             monitor=monitor, \n                             save_best_only=True, \n                             mode='max')\n    return checkpoint","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:53.008246Z","iopub.execute_input":"2023-07-16T13:08:53.008987Z","iopub.status.idle":"2023-07-16T13:08:53.026555Z","shell.execute_reply.started":"2023-07-16T13:08:53.008943Z","shell.execute_reply":"2023-07-16T13:08:53.024731Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"### Embedding Layer\nmax_vocab_length = 10000 \nmax_length =20\nfrom tensorflow.keras import layers\n\ntf.random.set_seed(42)\n\nembedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n                             output_dim=128, # set size of embedding vector\n                             embeddings_initializer=\"uniform\", # default, intialize randomly\n                             input_length=max_length, # how long is each input\n                             name=\"embedding_1\") \n\nembedding","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:53.029780Z","iopub.execute_input":"2023-07-16T13:08:53.030263Z","iopub.status.idle":"2023-07-16T13:08:53.147168Z","shell.execute_reply.started":"2023-07-16T13:08:53.030213Z","shell.execute_reply":"2023-07-16T13:08:53.145909Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"<keras.layers.core.embedding.Embedding at 0x7d3f7e24ff50>"},"metadata":{}}]},{"cell_type":"code","source":"# Setup text vectorization with custom variables\nmax_vocab_length = 10000 # max number of words to have in our vocabulary\nmax_length = 20 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n\ntext_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n                                    output_mode=\"int\",\n                                    output_sequence_length=max_length)\ntext_vectorizer.adapt(train_sentences)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:53.148471Z","iopub.execute_input":"2023-07-16T13:08:53.149749Z","iopub.status.idle":"2023-07-16T13:08:53.612793Z","shell.execute_reply.started":"2023-07-16T13:08:53.149698Z","shell.execute_reply":"2023-07-16T13:08:53.611276Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"import random\n# Get a random sentence from training set\nrandom_sentence = random.choice(train_sentences)\nprint(f\"Original text:\\n{random_sentence}\\\n      \\n\\nEmbedded version:\")\n\n# Embed the random sentence (turn it into numerical representation)\nsample_embed = embedding(text_vectorizer([random_sentence]))\nsample_embed","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:53.615025Z","iopub.execute_input":"2023-07-16T13:08:53.615389Z","iopub.status.idle":"2023-07-16T13:08:53.677866Z","shell.execute_reply.started":"2023-07-16T13:08:53.615353Z","shell.execute_reply":"2023-07-16T13:08:53.676489Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Original text:\ntexas seeks comment on rules for changes to windstorm insurer httptcoxqiadg9h2w httptcoypeelmjdzy      \n\nEmbedded version:\n","output_type":"stream"},{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"<tf.Tensor: shape=(1, 20, 128), dtype=float32, numpy=\narray([[[ 0.04160253, -0.03291667,  0.01157535, ...,  0.03335316,\n          0.04696257, -0.02926267],\n        [-0.04922062, -0.04021589, -0.04722719, ..., -0.02216609,\n         -0.00343936, -0.02892765],\n        [-0.00956619, -0.03614429,  0.03632928, ..., -0.00879533,\n         -0.01378595, -0.02361184],\n        ...,\n        [-0.0227595 , -0.03293587, -0.00569054, ...,  0.03492843,\n          0.00461795,  0.02796919],\n        [-0.0227595 , -0.03293587, -0.00569054, ...,  0.03492843,\n          0.00461795,  0.02796919],\n        [-0.0227595 , -0.03293587, -0.00569054, ...,  0.03492843,\n          0.00461795,  0.02796919]]], dtype=float32)>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Transfer Learning model form tfhub","metadata":{}},{"cell_type":"code","source":"import tensorflow_hub as hub\n# We can use this encoding layer in place of our text_vectorizer and embedding layer\nsentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n                                        input_shape=[], # shape of inputs coming to our model \n                                        dtype=tf.string, # data type of inputs coming to the USE layer\n                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n                                        name=\"USE\") ","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:08:53.681337Z","iopub.execute_input":"2023-07-16T13:08:53.681701Z","iopub.status.idle":"2023-07-16T13:08:59.215152Z","shell.execute_reply.started":"2023-07-16T13:08:53.681666Z","shell.execute_reply":"2023-07-16T13:08:59.213782Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# Create model using the Sequential API\nmodel = tf.keras.Sequential([\n  sentence_encoder_layer, # take in sentences and then encode them into an embedding\n\n  layers.Dense(128, activation=\"relu\"),\n  layers.Dense(64, activation=\"relu\"),   layers.Dense(32, activation=\"relu\"),  \n \n\n  layers.Dense(1, activation=\"sigmoid\",name='output')\n], name=\"Transfer_learning_model\")\n\n# Compile model\nmodel.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"],\n              )\nfor i in model.layers[:-11]:\n    i.trainable=True\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:21:18.794246Z","iopub.execute_input":"2023-07-16T13:21:18.795243Z","iopub.status.idle":"2023-07-16T13:21:18.974049Z","shell.execute_reply.started":"2023-07-16T13:21:18.795187Z","shell.execute_reply":"2023-07-16T13:21:18.972804Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"Model: \"Transfer_learning_model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n USE (KerasLayer)            (None, 512)               256797824 \n                                                                 \n dense_32 (Dense)            (None, 128)               65664     \n                                                                 \n dense_33 (Dense)            (None, 64)                8256      \n                                                                 \n dense_34 (Dense)            (None, 32)                2080      \n                                                                 \n output (Dense)              (None, 1)                 33        \n                                                                 \n=================================================================\nTotal params: 256,873,857\nTrainable params: 76,033\nNon-trainable params: 256,797,824\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model_history=model.fit(train_sentences,train_labels,\n                            validation_data=(val_sentences, val_labels),\n                        batch_size=64,\n#                              \n           epochs=25)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:21:22.351594Z","iopub.execute_input":"2023-07-16T13:21:22.352138Z","iopub.status.idle":"2023-07-16T13:21:55.397871Z","shell.execute_reply.started":"2023-07-16T13:21:22.352086Z","shell.execute_reply":"2023-07-16T13:21:55.396897Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"Epoch 1/25\n86/86 [==============================] - 4s 21ms/step - loss: 0.5123 - accuracy: 0.7540 - val_loss: 0.4226 - val_accuracy: 0.8097\nEpoch 2/25\n86/86 [==============================] - 1s 15ms/step - loss: 0.4026 - accuracy: 0.8263 - val_loss: 0.4131 - val_accuracy: 0.8123\nEpoch 3/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.3640 - accuracy: 0.8396 - val_loss: 0.4129 - val_accuracy: 0.8228\nEpoch 4/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.3289 - accuracy: 0.8635 - val_loss: 0.4188 - val_accuracy: 0.8163\nEpoch 5/25\n86/86 [==============================] - 1s 15ms/step - loss: 0.2796 - accuracy: 0.8885 - val_loss: 0.4400 - val_accuracy: 0.8163\nEpoch 6/25\n86/86 [==============================] - 1s 16ms/step - loss: 0.2191 - accuracy: 0.9190 - val_loss: 0.5193 - val_accuracy: 0.8058\nEpoch 7/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.1620 - accuracy: 0.9449 - val_loss: 0.5821 - val_accuracy: 0.8005\nEpoch 8/25\n86/86 [==============================] - 1s 17ms/step - loss: 0.1173 - accuracy: 0.9608 - val_loss: 0.6701 - val_accuracy: 0.7900\nEpoch 9/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0867 - accuracy: 0.9704 - val_loss: 0.7234 - val_accuracy: 0.7953\nEpoch 10/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0728 - accuracy: 0.9790 - val_loss: 0.7792 - val_accuracy: 0.7913\nEpoch 11/25\n86/86 [==============================] - 1s 13ms/step - loss: 0.0600 - accuracy: 0.9832 - val_loss: 0.8245 - val_accuracy: 0.7940\nEpoch 12/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0552 - accuracy: 0.9828 - val_loss: 0.8403 - val_accuracy: 0.7822\nEpoch 13/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0507 - accuracy: 0.9845 - val_loss: 0.8184 - val_accuracy: 0.8005\nEpoch 14/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0458 - accuracy: 0.9839 - val_loss: 0.8904 - val_accuracy: 0.7927\nEpoch 15/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0426 - accuracy: 0.9849 - val_loss: 0.8957 - val_accuracy: 0.7953\nEpoch 16/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0394 - accuracy: 0.9856 - val_loss: 0.8759 - val_accuracy: 0.8045\nEpoch 17/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0430 - accuracy: 0.9845 - val_loss: 0.9280 - val_accuracy: 0.7874\nEpoch 18/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0334 - accuracy: 0.9874 - val_loss: 0.9461 - val_accuracy: 0.8150\nEpoch 19/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0282 - accuracy: 0.9883 - val_loss: 1.0212 - val_accuracy: 0.7966\nEpoch 20/25\n86/86 [==============================] - 1s 13ms/step - loss: 0.0312 - accuracy: 0.9870 - val_loss: 0.9658 - val_accuracy: 0.7979\nEpoch 21/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0287 - accuracy: 0.9880 - val_loss: 0.9991 - val_accuracy: 0.7953\nEpoch 22/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0269 - accuracy: 0.9883 - val_loss: 1.0400 - val_accuracy: 0.7887\nEpoch 23/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0298 - accuracy: 0.9878 - val_loss: 0.9945 - val_accuracy: 0.7822\nEpoch 24/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0277 - accuracy: 0.9880 - val_loss: 1.0203 - val_accuracy: 0.7966\nEpoch 25/25\n86/86 [==============================] - 1s 14ms/step - loss: 0.0227 - accuracy: 0.9901 - val_loss: 1.0713 - val_accuracy: 0.7927\n","output_type":"stream"}]},{"cell_type":"code","source":"model_5.evaluate(test_sentences,test_labels)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:20:31.331558Z","iopub.execute_input":"2023-07-16T13:20:31.332002Z","iopub.status.idle":"2023-07-16T13:20:32.047111Z","shell.execute_reply.started":"2023-07-16T13:20:31.331952Z","shell.execute_reply":"2023-07-16T13:20:32.046047Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"43/43 [==============================] - 0s 10ms/step - loss: 1.6255 - accuracy: 0.7753\n","output_type":"stream"},{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"[1.625501275062561, 0.7753464579582214]"},"metadata":{}}]},{"cell_type":"code","source":"val_true=model_5.predict(test_sentences)\nval_true=tf.squeeze(tf.round(val_true))\nmodel_5_results=claclate_results_scores(val_true,test_labels)\nmodel_5_results","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:10:35.698409Z","iopub.execute_input":"2023-07-16T13:10:35.698758Z","iopub.status.idle":"2023-07-16T13:10:36.568152Z","shell.execute_reply.started":"2023-07-16T13:10:35.698726Z","shell.execute_reply":"2023-07-16T13:10:36.566873Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"43/43 [==============================] - 1s 8ms/step\n","output_type":"stream"},{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"{'model_accuarcy': 0.775346462436178,\n 'model_precision': 0.7757965666040388,\n 'model_recall': 0.775346462436178,\n 'model_f_score': 0.7755449127160925}"},"metadata":{}}]},{"cell_type":"code","source":"test_pred = model.predict([test_df['text']])\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-16T13:13:14.683713Z","iopub.execute_input":"2023-07-16T13:13:14.684158Z","iopub.status.idle":"2023-07-16T13:13:16.605531Z","shell.execute_reply.started":"2023-07-16T13:13:14.684120Z","shell.execute_reply":"2023-07-16T13:13:16.604176Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"102/102 [==============================] - 1s 8ms/step\n","output_type":"stream"}]}]}